{"cells":[{"metadata":{"_uuid":"7fad15a600d8a4b8a0e8a23fef765b8b578b0596"},"cell_type":"markdown","source":"# Introduction\nI had previously written a short kernel to practice Python. My \"native language\" is R, but the benefits of using Python are too great to ignore. In this notebook, I revist the same dataset but attempt several additional techniques, including: \n1. Pre-process with object-oriented programming\n2. Create Pipeline\n    * using custom transformers\n    * using decorators and a \"general pipleline class\" \n3. Use partial dependency plots\n4. running multiple aglorithms through a pipeline\n\nThe individual elements could be the subject af a kernel all to themselves, but I am choosing to make each occupy a section within this Kernel. I hope this helps you on your python journey! "},{"metadata":{"_uuid":"20a145f3146cd2976f0ab5a8c58ac148170b9710"},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nfrom subprocess import check_output\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.impute import MissingIndicator\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectPercentile, mutual_info_regression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import reciprocal, uniform\nfrom pdpbox import pdp, info_plots\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport functools","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"6c27632dd1c175006c86049bba665ae304d5068d"},"cell_type":"markdown","source":"# Dictionaries"},{"metadata":{"_uuid":"2c9977d2d8d7ae50e2a5129cf3f368e87e6cb6f9"},"cell_type":"markdown","source":"Below is a (nested?) dictionary which contains specific values for each column. This was initially performed by another user on Kaggle -- if someone happens to know who made these dicionaries initially then I will edit this notebook and provide proper attribution.  My contribution was coalesce all of the individual dictionaries into one. "},{"metadata":{"trusted":true,"_uuid":"de20ffb3ea9c86109148b3b202ce0e48f516436f"},"cell_type":"code","source":"replacement_dict = {\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"},\n            \"SaleCondition\" : {\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1},\n            \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n            \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n            \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n            \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n            \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n            \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n            \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n            \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n            \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n            \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4},\n            \"OverallQual\" : {1 : 1, 2 : 1, 3 : 1, # bad\n                           4 : 2, 5 : 2, 6 : 2, # average\n                           7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                          },\n            \"OverallCond\" : {1 : 1, 2 : 1, 3 : 1, # bad\n                                      4 : 2, 5 : 2, 6 : 2, # average\n                                      7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                     },\n    \"ModExterCond\" : {1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  },\n    \"ModExterQual\" : {1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  },\n    \"ModFunctional\" : {1 : 1, 2 : 1, # bad\n                                                     3 : 2, 4 : 2, # major\n                                                     5 : 3, 6 : 3, 7 : 3, # minor\n                                                     8 : 4 # typical\n                                                    },\n    \"ModKitchenQual\" : {1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      },\n    \"ModHeatingQC\" : {1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  }   \n}","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"b1b1c6122c57550ddaad078e394524e7c8bf34a0"},"cell_type":"markdown","source":"# 1. Pre-process with Object Oriented Programming"},{"metadata":{"_uuid":"319646cf9658217dd5c3a507168835e2451d7be4"},"cell_type":"markdown","source":"I have learned that there are many benefits to using OOP with Python. It makes code more readable, reproducible, testable, etc. For this section, I create a python class specifically for this competition to handle the \"pre-processing\" of the data. What I mean by \"pre-processing\" is making the training and testing sets equivalent in form. This class contains functions to: \n1. Load the training and testing data\n2. Generate a \"submission\" dataframe\n3. Use the above dictionary to replace vlalues\n4. Create new columns by:\n    * multiplication\n    * addition\n    * hardcoding\n5. Log transforming the response\n6. Removing categories that do not show up in Test (to allow for one-hot-encoding)"},{"metadata":{"trusted":true,"_uuid":"e797274dd9e4df5c89f26f4c6614917a15308b4f"},"cell_type":"code","source":"# Define Class\nclass HousingPricesRegression():\n    # class initialization\n    def __init__(self, train_path, test_path):\n        self.train = pd.read_csv(train_path)\n        self.test = pd.read_csv(test_path)\n        self.submission = pd.DataFrame()\n    def gen_submission_file(self, id_col):\n        self.submission[id_col] = self.test[id_col]\n        self.test.drop([id_col], axis = 1, inplace=True)\n    # replace information within columns\n    def replace_values(self, replace_dict):\n        # Find name (top level key)\n        key_name = str(*replace_dict)\n        true_index = np.where(self.train.columns.isin([key_name]))\n        col_name = list(self.train.columns[true_index])\n        if len(col_name) == 0:\n            # Option 1: Dictionary refers to a key that doesn't exist. Create one!\n            orig_name = key_name[3:]\n            self.train[key_name] = self.train[orig_name].map(replace_dict[key_name])\n            self.test[key_name] = self.test[orig_name].map(replace_dict[key_name])\n        else:\n            # Option 2: Dictionary refers to a key that does exist. Modify! \n            # Take 'ModExterCond' and find the original (\"ExterCond\")\n            self.train[key_name] = self.train[key_name].map(replace_dict[key_name])\n            self.test[key_name] = self.test[key_name].map(replace_dict[key_name])\n    # Take Logarithm of Response\n    def create_response(self, response):\n        self.price_labels = np.log(self.train[response].copy() + 1)\n        # drop original to avoid confusion\n        self.train.drop([response], axis=1, inplace = True)  \n    # Remove categories \n    def remove_categories(self, cat_cols):\n        for i in cat_cols:\n            trn = self.train[i].astype('category')\n            trn = trn.cat.categories\n            tst = self.test[i].astype('category')\n            tst = tst.cat.categories\n            # Find that which is in test which is not in train\n            unique_values_train = list(set(trn) - set(tst))\n            # Check \n            # If no difference, print\n            if len(unique_values_train)==0:\n                print(\"No Differnence for {}\".format(i))\n            else:\n                # If there is a differnce, change to NA in test for later imputation\n                for j in unique_values_train:\n                    to_replace_index = self.train.loc[self.train[i]==j,i].index\n                    self.train.loc[to_replace_index,i] = float('NaN')\n                    self.train[i] = self.train[i].cat.remove_categories(j)\n                    print(\"{} removed from {} in train\".format(j,i))\n    # Set the category type \n    def make_category(self, cat_cols):\n        self.train[cat_cols] = self.train[cat_cols].astype('category')\n        self.test[cat_cols] = self.test[cat_cols].astype('category')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"6c7cd3b831c86b1d5bc7d462ff4667aa396ad86b"},"cell_type":"markdown","source":"#### Load Data"},{"metadata":{"trusted":true,"_uuid":"c95568a9daaf3929f3a01acc609126a33224e3d9"},"cell_type":"code","source":"# Define Paths\nfile_path = '../input/'\ntrain_path = str(file_path) + str('train.csv')\ntest_path = str(file_path) + str('test.csv')\nsubmission_path = str(file_path) + str()\n\n# Instantiate model object\nhprObject = HousingPricesRegression(train_path, test_path)\n\n# Generate submission file\nhprObject.gen_submission_file(id_col='Id')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"ec7b7593724c1c793f467bd3be78fbe89e3c509c"},"cell_type":"markdown","source":"#### Call Replacement Method\n\nIt's possible to get the key-names of a dictionary using `*dict`. This block of code loops through `replacement_dict`, essentially creating a subset dictionary. This subset is then passed to the `replace_values()` method, which in turn grabs the top level key (the column name) and attempts to find all of the levels to replace, which is accomplished by `.map()`. Note that the train and test data are pandas dataframes at this point. "},{"metadata":{"trusted":true,"_uuid":"dee1661cdf3ed6fb93269b0466dc33f42f5338b6"},"cell_type":"code","source":"# Replace features - multiple columns\nfor key, values in replacement_dict.items():\n    temp_dict = {}\n    temp_dict[key] = replacement_dict[key] \n    hprObject.replace_values(temp_dict)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"ffe40578d6389a7e3b936071f9b70b35f6e17443"},"cell_type":"markdown","source":"#### Create Response\nThis creates a single column dataframe called `price_labels` that is a copy of that in train, except log-transformed. The corresponding column in the train dataframe is also dropped. "},{"metadata":{"trusted":true,"_uuid":"4fb26d688b4bd86af2e3b20b9dfb9f9abf036cc8"},"cell_type":"code","source":"# This takes the log of the sales price\nhprObject.create_response(response=\"SalePrice\")","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"f3dd451a99239b17fd5ec80168b44975df67a242"},"cell_type":"markdown","source":"#### Equivocate Training and Test Sets\nI had trouble with this for a while. Apparently, there are 'levels' in a given column in train that do not exist in test. For some reason, I have a problem with one-hot encoding if I do *not* reconcile this. What I do now is simply make that particular 'cell' (a specific row in a specific column) blank. *I do not delete the row*. Later in the kernel I impute for blanks. "},{"metadata":{"trusted":true,"_uuid":"c9550b2e545818a66b43ee5087ab03906b379d52"},"cell_type":"code","source":"## Create list of numerical and categorical attributes\nnum_cols = hprObject.test._get_numeric_data().columns\ncat_cols = list(set(hprObject.test.columns) - set(num_cols))\nnum_cols = list(num_cols)\n\n## Need to move this to a class method or something\nhprObject.make_category(cat_cols)\n\n# Replace unique levels in test with NaN to be imputed later\nhprObject.remove_categories(cat_cols)\n\n## Make sure that the training data only reflects the columns in the testing data \nnames_in_test = list(hprObject.test)\nhprObject.train = hprObject.train[names_in_test]","execution_count":7,"outputs":[{"output_type":"stream","text":"No Differnence for Neighborhood\nNo Differnence for GarageCond\nNo Differnence for LandContour\nTenC removed from MiscFeature in train\nNo Differnence for BsmtQual\nNo Differnence for CentralAir\nNo Differnence for BsmtFinType1\nNo Differnence for MSZoning\nNo Differnence for FireplaceQu\nNo Differnence for LotConfig\nNo Differnence for Foundation\nNo Differnence for Fence\nNo Differnence for RoofStyle\nNo Differnence for MasVnrType\nNo Differnence for BsmtFinType2\nFa removed from PoolQC in train\nImStucc removed from Exterior1st in train\nStone removed from Exterior1st in train\nNo Differnence for BsmtCond\nRoll removed from RoofMatl in train\nMetal removed from RoofMatl in train\nClyTile removed from RoofMatl in train\nMembran removed from RoofMatl in train\n2.5Fin removed from HouseStyle in train\nRRAe removed from Condition2 in train\nRRAn removed from Condition2 in train\nRRNn removed from Condition2 in train\nNo Differnence for MoSold\nEx removed from GarageQual in train\nNo Differnence for BldgType\nNo Differnence for GarageType\nNo Differnence for SaleType\nNo Differnence for BsmtExposure\nNo Differnence for Condition1\nOther removed from Exterior2nd in train\nNo Differnence for Alley\nMix removed from Electrical in train\nOthW removed from Heating in train\nFloor removed from Heating in train\nNo Differnence for GarageFinish\n","name":"stdout"}]},{"metadata":{"_uuid":"0c50127e31356b92d373b08b9a066c6fc2f4763a"},"cell_type":"markdown","source":"## Make Pipeline\n### Custom Transformers"},{"metadata":{"trusted":true,"_uuid":"4c91edaf5c5c8531b416a189c7f747abcac12251"},"cell_type":"code","source":"# Custom transformer only multiplies two columns at a time\nclass Multiply(BaseEstimator, TransformerMixin):\n    def __init__(self, new_name):\n        self.name_list = None\n        self.new_name = new_name\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        self.name_list=X.columns\n        X[self.new_name] = X[self.name_list[0]] * X[self.name_list[1]]\n        return X\n    def fit_transform(self, X, y=None):\n        return self.fit(X,y).transform(X)\n    \n## Custom transformer adds any number of columns at a time\nclass Add(BaseEstimator, TransformerMixin):\n    def __init__(self, new_name):\n        self.name_list = None\n        self.new_name = new_name\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        self.name_list=X.columns\n        X[self.new_name] = X.sum(axis=1)\n        return X\n    def fit_transform(self, X, y=None):\n        return self.fit(X,y).transform(X)\n\n## Custom transformer specific to bathroom\nclass CountBath(BaseEstimator, TransformerMixin):\n    def __init__(self, new_name):\n        self.name_list = None\n        self.new_name = new_name\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        self.name_list=X.columns\n        X[self.new_name] = X[\"BsmtFullBath\"] + X[\"FullBath\"]+ (0.5 * (X[\"BsmtHalfBath\"]+ X[\"HalfBath\"]))\n        return X\n    def fit_transform(self, X, y=None):\n        return self.fit(X,y).transform(X)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Column Names ##\nkitchen_col = ['KitchenAbvGr', 'KitchenQual']\ngrade_col = ['OverallQual', 'OverallCond']\nexter_cond = ['ExterQual','ExterCond']\nadd_area = ['GrLivArea','TotalBsmtSF']\nadd_floor = ['1stFlrSF', '2ndFlrSF']\nadd_porch = ['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']\ncount_bath = ['BsmtFullBath','FullBath','BsmtHalfBath','HalfBath']\nnew_names = ['KitchenTotal','OverallTotal','ExterTotal','AreaTotal','FloorTotal','PorchTotal','BathTotal']\n\nfeature_num = kitchen_col + grade_col + exter_cond + add_area + add_floor + add_porch + count_bath + new_names\nother_num_cols = list(set(num_cols) - set(feature_num))\n\n## Column Transformers ##\nfeature_engineering = ColumnTransformer(\n    transformers = [\n        ('multi_kitchen', Multiply(new_name=new_names[0]), kitchen_col),\n        ('multi_grade', Multiply(new_name=new_names[1]), grade_col),\n        ('multi_exter', Multiply(new_name=new_names[2]), exter_cond),\n        ('add_total_SF', Add(new_name=new_names[3]), add_area),\n        ('add_floor_SF', Add(new_name=new_names[4]), add_floor),\n        ('add_porch_SF', Add(new_name=new_names[5]), add_porch),\n        ('count_baths', CountBath(new_name=new_names[6]), count_bath)\n    ],\n    remainder='drop'\n)\n\nother_numerical = ColumnTransformer(\n    transformers = [\n        ('impute_other', SimpleImputer(strategy=\"median\"), other_num_cols),\n        ('scale_other', StandardScaler(), other_num_cols)\n    ],\n    remainder='drop'\n)\n\ncategorical = ColumnTransformer(\n    transformers = [\n        ('impute_cats', SimpleImputer(strategy=\"most_frequent\"), cat_cols),\n        \n    ],\n    remainder='drop'\n)\n\nother_missing = ColumnTransformer(\n    transformers = [\n        ('other_missing', MissingIndicator(features='all'), other_num_cols)\n    ]\n)\n\ncategory_missing = ColumnTransformer(\n    transformers = [\n        ('cat_missing', MissingIndicator(features='all'), cat_cols)\n    ]\n)\n\n## Pipelines ## \nstep1 = Pipeline([\n    ('feat_eng', feature_engineering),\n    ('impute_feats', SimpleImputer(strategy=\"median\")),\n    ('scale_feats', StandardScaler())\n])\n\nstep2 = Pipeline([\n    ('other_nums', other_numerical),\n    ('impute_nums', SimpleImputer(strategy=\"median\")),\n    ('scale_nums', StandardScaler())\n])\n\nstep3 = Pipeline([\n    ('categ', categorical),\n    ('encode', OneHotEncoder(handle_unknown='ignore'))\n])\n        \nunify = FeatureUnion([\n    ('new_features', step1 ),\n    ('numeric_scale', step2), \n    ('categoric_encode', step3),\n    ('num_missing_indicator', other_missing),\n    ('cat_missing_indicator', category_missing)\n])\n\nfinal_pipeline = Pipeline([\n    ('finally', unify)\n])\n\n## Just checking\n#f = final_pipeline.fit_transform(hprObject.train)\n#f.shape","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train into train and validation set\nX_train, X_val, y_train, y_val = train_test_split(hprObject.train, hprObject.price_labels, test_size = 0.2)\n\n# list names of regressors\nnames = [\n         \"Support-Vector Regressor\",\n         \"Random Forest Regressor\"\n        ]\n\nregressors = [\n    SVR(),\n    RandomForestRegressor()\n]\n\nparameters = [\n                  {\n                      'reg__C': (reciprocal(0.001, 0.1)), \n                      'reg__gamma': (uniform(1, 10))\n                  },\n                  {\n                      'reg__max_depth': (5,10)\n                  }\n             ]","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = {}\nfor name, regressor, params in zip(names, regressors, parameters):\n    reg_pipe = Pipeline([\n        ('pipeline', final_pipeline),\n        ('reg', regressor),\n    ])\n    rs_reg = RandomizedSearchCV(reg_pipe, params, n_iter=10, verbose=2, random_state=42, scoring='neg_mean_squared_error')\n    regres = rs_reg.fit(X_train, y_train)\n    score = regres.score(X_val, y_val)\n    print(\"{} score: {}\".format(name, score))\n    model_list[name] = regres","execution_count":21,"outputs":[{"output_type":"stream","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] reg__C=0.005611516415334503, reg__gamma=10.50714306409916 .......\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"[CV]  reg__C=0.005611516415334503, reg__gamma=10.50714306409916, total=   0.5s\n[CV] reg__C=0.005611516415334503, reg__gamma=10.50714306409916 .......\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV]  reg__C=0.005611516415334503, reg__gamma=10.50714306409916, total=   0.5s\n[CV] reg__C=0.005611516415334503, reg__gamma=10.50714306409916 .......\n[CV]  reg__C=0.005611516415334503, reg__gamma=10.50714306409916, total=   0.4s\n[CV] reg__C=0.029106359131330688, reg__gamma=6.986584841970366 .......\n[CV]  reg__C=0.029106359131330688, reg__gamma=6.986584841970366, total=   0.5s\n[CV] reg__C=0.029106359131330688, reg__gamma=6.986584841970366 .......\n[CV]  reg__C=0.029106359131330688, reg__gamma=6.986584841970366, total=   0.5s\n[CV] reg__C=0.029106359131330688, reg__gamma=6.986584841970366 .......\n[CV]  reg__C=0.029106359131330688, reg__gamma=6.986584841970366, total=   0.5s\n[CV] reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267 .....\n[CV]  reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267, total=   0.5s\n[CV] reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267 .....\n[CV]  reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267, total=   0.4s\n[CV] reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267 .....\n[CV]  reg__C=0.0020513382630874496, reg__gamma=2.5599452033620267, total=   0.4s\n[CV] reg__C=0.0013066739238053278, reg__gamma=9.661761457749352 ......\n[CV]  reg__C=0.0013066739238053278, reg__gamma=9.661761457749352, total=   0.4s\n[CV] reg__C=0.0013066739238053278, reg__gamma=9.661761457749352 ......\n[CV]  reg__C=0.0013066739238053278, reg__gamma=9.661761457749352, total=   0.4s\n[CV] reg__C=0.0013066739238053278, reg__gamma=9.661761457749352 ......\n[CV]  reg__C=0.0013066739238053278, reg__gamma=9.661761457749352, total=   0.4s\n[CV] reg__C=0.01593052261624101, reg__gamma=8.080725777960454 ........\n[CV]  reg__C=0.01593052261624101, reg__gamma=8.080725777960454, total=   0.4s\n[CV] reg__C=0.01593052261624101, reg__gamma=8.080725777960454 ........\n[CV]  reg__C=0.01593052261624101, reg__gamma=8.080725777960454, total=   0.4s\n[CV] reg__C=0.01593052261624101, reg__gamma=8.080725777960454 ........\n[CV]  reg__C=0.01593052261624101, reg__gamma=8.080725777960454, total=   0.4s\n[CV] reg__C=0.0010994335574766197, reg__gamma=10.699098521619943 .....\n[CV]  reg__C=0.0010994335574766197, reg__gamma=10.699098521619943, total=   0.4s\n[CV] reg__C=0.0010994335574766197, reg__gamma=10.699098521619943 .....\n[CV]  reg__C=0.0010994335574766197, reg__gamma=10.699098521619943, total=   0.4s\n[CV] reg__C=0.0010994335574766197, reg__gamma=10.699098521619943 .....\n[CV]  reg__C=0.0010994335574766197, reg__gamma=10.699098521619943, total=   0.4s\n[CV] reg__C=0.046225890010208284, reg__gamma=3.1233911067827616 ......\n[CV]  reg__C=0.046225890010208284, reg__gamma=3.1233911067827616, total=   0.4s\n[CV] reg__C=0.046225890010208284, reg__gamma=3.1233911067827616 ......\n[CV]  reg__C=0.046225890010208284, reg__gamma=3.1233911067827616, total=   0.4s\n[CV] reg__C=0.046225890010208284, reg__gamma=3.1233911067827616 ......\n[CV]  reg__C=0.046225890010208284, reg__gamma=3.1233911067827616, total=   0.4s\n[CV] reg__C=0.002310201887845293, reg__gamma=2.834045098534338 .......\n[CV]  reg__C=0.002310201887845293, reg__gamma=2.834045098534338, total=   0.4s\n[CV] reg__C=0.002310201887845293, reg__gamma=2.834045098534338 .......\n[CV]  reg__C=0.002310201887845293, reg__gamma=2.834045098534338, total=   0.4s\n[CV] reg__C=0.002310201887845293, reg__gamma=2.834045098534338 .......\n[CV]  reg__C=0.002310201887845293, reg__gamma=2.834045098534338, total=   0.4s\n[CV] reg__C=0.004059611610484304, reg__gamma=6.247564316322379 .......\n[CV]  reg__C=0.004059611610484304, reg__gamma=6.247564316322379, total=   0.4s\n[CV] reg__C=0.004059611610484304, reg__gamma=6.247564316322379 .......\n[CV]  reg__C=0.004059611610484304, reg__gamma=6.247564316322379, total=   0.4s\n[CV] reg__C=0.004059611610484304, reg__gamma=6.247564316322379 .......\n[CV]  reg__C=0.004059611610484304, reg__gamma=6.247564316322379, total=   0.4s\n[CV] reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194 .....\n[CV]  reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194, total=   0.4s\n[CV] reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194 .....\n[CV]  reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194, total=   0.4s\n[CV] reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194 .....\n[CV]  reg__C=0.0073095398359129095, reg__gamma=3.9122914019804194, total=   0.4s\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   20.2s finished\n","name":"stderr"},{"output_type":"stream","text":"Support-Vector Regressor score: -0.16445892822590957\nFitting 3 folds for each of 2 candidates, totalling 6 fits\n[CV] reg__max_depth=5 ................................................\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"[CV] ................................. reg__max_depth=5, total=   0.3s\n[CV] reg__max_depth=5 ................................................\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n","name":"stderr"},{"output_type":"stream","text":"[CV] ................................. reg__max_depth=5, total=   0.3s\n[CV] reg__max_depth=5 ................................................\n[CV] ................................. reg__max_depth=5, total=   0.3s\n[CV] reg__max_depth=10 ...............................................\n[CV] ................................ reg__max_depth=10, total=   0.7s\n[CV] reg__max_depth=10 ...............................................\n[CV] ................................ reg__max_depth=10, total=   0.6s\n[CV] reg__max_depth=10 ...............................................\n[CV] ................................ reg__max_depth=10, total=   0.7s\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    3.3s finished\n","name":"stderr"},{"output_type":"stream","text":"Random Forest Regressor score: -0.02927283976289624\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list['Support-Vector Regressor'].best_params_","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"{'reg__C': 0.01593052261624101, 'reg__gamma': 8.080725777960454}"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"555afb474fdea96cfecaba1d5d48ef1adf35e6bc"},"cell_type":"code","source":"perm = PermutationImportance(svr_model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = feature_name_list, top = 58)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb1651541df46dc1b8c195e1f43868029a636f9"},"cell_type":"markdown","source":"# Generate Partial Dependency Plots"},{"metadata":{"trusted":true,"_uuid":"ab7e10f246821fec13a00072f58e254f7f0fa9ef"},"cell_type":"code","source":"# Names of Numeric features\nnum_cols = hprObject.train._get_numeric_data().columns\nfeature_name_list = list(num_cols)\n\n# Select numerical features\nnum_train_df = hprObject.train[feature_name_list]\n\n# Numeric Pipeline\nnumerical_pipeline = Pipeline([\n   # ('drop_columns', DropMissing(threshold_percent = 0.07)),\n    ('select_numeric', TypeSelector(np.number)),\n    ('impute_missing', SimpleImputer(strategy = \"median\")),\n    ('std_scaler', StandardScaler()),\n    ('select_features', SelectPercentile(mutual_info_regression, percentile=95))\n])\n\n# Run through pipeline\nnum_train_np = numerical_pipeline.fit_transform(X = num_train_df, y = hprObject.price_labels)\n\n# Split \nX_train, X_val, y_train, y_val = train_test_split(num_train_np, hprObject.price_labels, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eab522ea26b0964f4c713a83294f815aa858d8f6"},"cell_type":"code","source":"# Make dataframe copy of X_Val\nX_val_df = pd.DataFrame(X_val)\nX_val_df.columns = feature_name_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63538c49ed8b92d2020bdd58a2ad9ed919b8376f"},"cell_type":"code","source":"# Perform cross-validation\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, random_state=42)\nrnd_search_cv.fit(X_train, y_train)\n\n# Fit with optimal Parameters\nsvr_model = SVR(gamma = rnd_search_cv.best_params_[\"gamma\"], C = rnd_search_cv.best_params_[\"C\"])\nsvr_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b375fdee8ac70b0ff4e92a1f180303410607a31a"},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true,"_uuid":"c6b5153176c13fae78c8c5442aa1e7b5a149862b"},"cell_type":"code","source":"# Feature importance\nperm = PermutationImportance(svr_model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = feature_name_list, top = 58)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2288c03cd91cad5113672541bb2f1f62cece70d2"},"cell_type":"markdown","source":"## Partial Dependency Plots"},{"metadata":{"trusted":true,"_uuid":"c15c779691feee7f8ebef4f19972377466a67998"},"cell_type":"code","source":"# Create a series of partial depedency plots\nname = 'ModHeatingQC'\npdp_housing = pdp.pdp_isolate(model=svr_model, dataset=X_val_df, model_features=feature_name_list, feature=name)\n\n# plot it\npdp.pdp_plot(pdp_housing, name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b047fa161a346ac989ad84770e4de4839adda7f"},"cell_type":"code","source":"# Create a series of partial depedency plots\nname='HeatingQC'\npdp_housing = pdp.pdp_isolate(model=svr_model, dataset=X_val_df, model_features=feature_name_list, feature=name)\n\n# plot it\npdp.pdp_plot(pdp_housing, name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01f06f004590e2161b5d7b007aa0729120d8bfc4"},"cell_type":"markdown","source":"## Fit Data with Model(s) & Parameters"},{"metadata":{"trusted":true,"_uuid":"ee6fc950a2392225017325ac6c7f74315f34f9ea"},"cell_type":"code","source":"# split train into train and validation sets\nTrainX, ValX, TrainY, ValY = train_test_split(hprObject.train, hprObject.price_labels, test_size = 0.2)\n\n# run full pipeline\ntrain_data = unify_pipeline.fit_transform(TrainX, TrainY)\n\n# Run SVR()\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nsvr_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, random_state=42)\nsvr_cv.fit(train_data, TrainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3578bddc55efb2328c74fd86c496f81815cfcc22"},"cell_type":"code","source":"svr_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70359137543b5cf4d4c2b45073a59e561068665c"},"cell_type":"code","source":"# Run pipeline for validation data\ntest_data = unify_pipeline.transform(ValX, ValY)\n\n# Generate predictions to compare between models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc6c3f387566d9520c3424bc4429e0a89643a1a9"},"cell_type":"markdown","source":"## Try Running Multiple Models"},{"metadata":{"trusted":true,"_uuid":"a4478950f9140f6ae40675763a3ed9d77e8e3e00"},"cell_type":"code","source":"final_pipeline.get_params()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8a118c394e7576aae81b2c85634b02b2932a1b"},"cell_type":"markdown","source":"## Trying something different"},{"metadata":{"trusted":true,"_uuid":"9c9ecbbc97b9bdf7475784500be1982f021cf4ab"},"cell_type":"code","source":"reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a041e93a9736cae6db4444249b1849f08302e934"},"cell_type":"markdown","source":"## Generate Submission File"},{"metadata":{"trusted":true,"_uuid":"4fbf36d06564d7fa2ae9432ee19da07d3ffeeaa9"},"cell_type":"code","source":"# create submission\ny_pred = rnd_search_cv.best_estimator_.predict(train_prepared)\nmse = mean_squared_error(price_labels, y_pred)\nnp.exp(np.sqrt(mse))\n\nfitted_model = SVR().fit(X=prepared_train, y=hprObject.price_labels)\npredictions = fitted_model.predict(prepared_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}